
@article{https://doi.org/10.1002/cpe.4851,
author = {Bernholdt, David E. and Boehm, Swen and Bosilca, George and Gorentla Venkata, Manjunath and Grant, Ryan E. and Naughton, Thomas and Pritchard, Howard P. and Schulz, Martin and Vallee, Geoffroy R.},
title = {A survey of MPI usage in the US exascale computing project},
journal = {Concurrency and Computation: Practice and Experience},
volume = {32},
number = {3},
pages = {e4851},
keywords = {exascale, MPI},
doi = {https://doi.org/10.1002/cpe.4851},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.4851},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.4851},
note = {e4851 cpe.4851},
abstract = {Summary The Exascale Computing Project (ECP) is currently the primary effort in the United States focused on developing âexascaleâ levels of computing capabilities, including hardware, software, and applications. In order to obtain a more thorough understanding of how the software projects under the ECP are using, and planning to use the Message Passing Interface (MPI), and help guide the work of our own project within the ECP, we created a survey. Of the 97 ECP projects active at the time the survey was distributed, we received 77 responses, 56 of which reported that their projects were using MPI. This paper reports the results of that survey for the benefit of the broader community of MPI developers.},
year = {2020}
}

@article{osti-1462877,
title = "{A Survey of MPI Usage in the U.S. Exascale Computing Project}",
author = {David E. Bernholdt and Swen Boehm and George Bosilca and Manjunath Gorentla Venkata and Ryan E. Grant and Thomas J. Naughton and Howard P. Pritchard and Martin Schulz and Geoffroy R. Vallee},
abstractNote = {The Exascale Computing Project (ECP) is currently the primary effort in the United States focused on developing “exascale” levels of computing capability, including hardware, software and applications. In order to obtain a more thorough understanding of how the software projects under the ECP are using, and planning to use the Message Passing Interface (MPI), and help guide the work of our own project within the ECP, we created a survey. Of the 97 ECP projects active at the time the survey was distributed, we received 77 responses, 56 of which reported that their projects were using MPI. This paper reports the results of that survey for the benefit of the broader community of MPI developers.},
journal = {},
place = {United States},
year = {2018},
month = {June}
}
\bibitem[DATASET]{DATASET}
  \bibinfo{author}{A. Hori},
  \bibinfo{author}{E. Jeanbnot},
  \bibinfo{author}{G. Bosilca},
  \bibinfo{author}{T. Ogura},
  \bibinfo{title}{MPI International Survey Data (CSV)},
  \bibinfo{repository}{\url{https://github.com/bosilca/MPIsurvey.git}},
  \bibinfo{year}{``2021''}.


@misc{DATASET,
title = "{MPI International Survey -- GitHub Repository}",
author = {A. Hori and T. Ogura and E. Jeannot and G. Bosilca},
howpublished = "\url{https://exascaleproject.org/}",
year="2021"
}

@misc{ECP,
title = "{Exascale Computing Project}",
author = {{Exascale Computing Project}},
howpublished = "\url{https://exascaleproject.org/}",
year="2021"
}

@misc{hpci-user-survey,
jtitle = "{第4回「京」を中核とするHPCIシステムに関する利用者アンケート結果}",
title = "{Report of the fourth survey on the K computer and the other HPCI systems}",
author = {{RIST}},
year = {2018},
howpublished = "\url{http://www.hpci-office.jp/materials/k_chosa_4th}",
note = "(in Japanese)"
}

@misc{HPCI,
title = "{High-Performance Computing Infrastructure}",
author = {{Research Organization for Information Science and Technology (RIST)}},
howpublished = "\url{http://www.hpci-office.jp/folders/english}",
year="2018"
}

@misc{JLESC,
title = "{Joint Laboratories for Extreme-scale Computing}",
author = {{JLESC}},
howpublished = "\url{https://jlesc.github.io/}",
year="2021"
}

@misc{Top500,
title = "{Top 500}",
author = {{TOP500.org}},
howpublished = "\url{https://www.top500.org}",
year="2021"
}

@misc{mpi-forum,
title = "{MPI Forum}",
author = "{MPI Forum}",
howpublished = "\url{https://www.mpi-forum.org}"
}

@misc{mpi-tutorial,
title = "{A Comprehensive MPI Tutorial Resource}",
author = "Wes Kendall and Dwaraka Nath and Wesley Bland",
howpublished = "\url{https://mpitutorial.com}",
year="2021"
}

@misc{mpi-tutorial-intro,
title = "{MPI Tutorial Introduction}",
author = "Wes Kendall",
howpublished = "\url{https://mpitutorial.com/tutorials/mpi-introduction/}",
year="2021"
}

@misc{madmpi,
title = "{NewMadelein}",
author = "Alexandre Denis",
howpublished = "\url{http://pm2.gforge.inria.fr/newmadeleine}"
}

@book{mpi-hardcover,
title = "{MPI: A Message-Passing Interface Standard, Version 3.1}",
author = "{Message Passing Interface Forum}",
publisher = "{High Performance Computing Center Stuttgart (HLRS)}",
year = "2015"
}

@techreport{swopp2019,
   author	 = "Atsushi Hori and George Bosilca and Emmanuel Jeannot and Takahiro Ogura and Yutaka Ishikawa",
   title	 = "Is Japanese HPC another Galapagos? - Interim Report of MPI International Survey -",
   year 	 = "2019",
   number	 = "34",
   month	 = "Jul",
   institution = "Information Processing Society of Japan, SIGHPC",
howpublished = "\url{https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&page_id=13&block_id=8&item_id=198171&item_no=1}"
}

@inproceedings{10.1145/3295500.3356176,
author = {Laguna, Ignacio and Marshall, Ryan and Mohror, Kathryn and
                  Ruefenacht, Martin and Skjellum, Anthony and
                  Sultana, Nawrin}, 
title = {A Large-Scale Study of MPI Usage in Open-Source HPC Applications},
year = {2019},
isbn = {9781450362290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295500.3356176},
doi = {10.1145/3295500.3356176},
abstract = {Understanding the state-of-the-practice in MPI
                  usage is paramount for many aspects of
                  supercomputing, including optimizing the
                  communication of HPC applications and informing
                  standardization bodies and HPC systems procurements
                  regarding the most important MPI
                  features. Unfortunately, no previous study has
                  characterized the use of MPI on applications at a
                  significant scale; previous surveys focus either on
                  small data samples or on MPI jobs of specific HPC
                  centers. This paper presents the first comprehensive
                  study of MPI usage in applications. We survey more
                  than one hundred distinct MPI programs covering a
                  significantly large space of the population of MPI
                  applications. We focus on understanding the
                  characteristics of MPI usage with respect to the
                  most used features, code complexity, and programming
                  models and languages. Our study corroborates certain
                  findings previously reported on smaller data samples
                  and presents a number of interesting, previously
                  un-reported insights.},
booktitle = {Proceedings of the International Conference for High Performance
                  Computing, Networking, Storage and Analysis},
articleno = {31},
numpages = {14},
keywords = {program analysis, applications survey, MPI},
location = {Denver, Colorado},
series = {SC '19} }  

@INPROCEEDINGS{8665758,
  author={S. {Chunduri} and S. {Parker} and P. {Balaji} and K. {Harms} and K. {Kumaran}},
  booktitle={SC18: International Conference for High Performance Computing, Networking, Storage and Analysis}, 
  title={Characterization of MPI Usage on a Production Supercomputer}, 
  year={2018},
  volume={},
  number={},
  pages={386-400},
  doi={10.1109/SC.2018.00033}}

@inproceedings{10.1109/SC.2018.00033,
author = {Chunduri, Sudheer and
                  Parker, Scott and Balaji, Pavan and Harms, Kevin and
                  Kumaran, Kalyan},
title = {Characterization of MPI Usage on a Production Supercomputer},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC.2018.00033},
doi = {10.1109/SC.2018.00033},
abstract = {MPI is the most
                  prominent programming model used in scientific
                  computing today. Despite the importance of MPI,
                  however, how scientific computing applications use
                  it in production is not well understood. This lack
                  of understanding is attributed primarily to the fact
                  that production systems are often wary of
                  incorporating automatic profiling tools that perform
                  such analysis because of concerns about potential
                  performance overheads. In this study, we used a
                  lightweight profiling tool, called Autoperf, to log
                  the MPI usage characteristics of production
                  applications on a large IBM BG/Q supercomputing
                  system (Mira) and its corresponding development
                  system (Cetus). Autoperf limits the amount of
                  information that it records, in order to keep the
                  overhead to a minimum while still storing enough
                  data to derive useful insights. MPI usage statistics
                  have been collected for over 100K jobs that were run
                  within a two-year period and are analyzed in this
                  paper. The analysis is intended to provide useful
                  insights for MPI developers and network hardware
                  developers for their next generation of improvements
                  and for supercomputing center operators for their
                  next system procurements.},
booktitle = {Proceedings
                  of the International Conference for High Performance
                  Computing, Networking, Storage, and Analysis},
articleno = {30},
numpages = {15},
keywords = {autoperf, core-hours, monitoring, MPI},
location = {Dallas, Texas}, series = {SC '18} } 

@article{https://doi.org/10.1002/cpe.5901,
author = {Sultana, Nawrin and Rﾃｼfenacht, Martin and Skjellum, Anthony and Bangalore, Purushotham and Laguna, Ignacio and Mohror, Kathryn},
title = {Understanding the use of message passing interface in exascale proxy applications},
journal = {Concurrency and Computation: Practice and Experience},
volume = {n/a},
number = {n/a},
pages = {e5901},
keywords = {characterization, empirical analysis, exascale, MPI, profiling},
doi = {https://doi.org/10.1002/cpe.5901},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.5901},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.5901},
abstract = {Summary The Exascale Computing Project (ECP) focuses on the development of future exascale-capable applications. Most ECP applications use the message passing interface (MPI) as their parallel programming model with mini-apps serving as proxies. This paper explores the explicit usage of MPI in such ECP proxy applications. We empirically analyze 14 proxy applications from the ECP Proxy Apps Suite. We use the MPI profiling interface (PMPI) to collect MPI usage patterns in ECP proxy apps. Our analysis shows that a small subset of features from MPI is commonly used in the proxies of exascale-capable applications, even when they reference third-party libraries. This study is intended to provide a better understanding of the use of MPI in current exascale applications. The findings can help focus software investments made for exascale systems in the MPI middleware including optimization, fault-tolerance, tuning, and hardware-offload.},
year="2020"
}

@InProceedings{10.1007/978-3-319-58667-0-12,
author="Klenk, Benjamin
and Fr{\"o}ning, Holger",
editor="Kunkel, Julian M.
and Yokota, Rio
and Balaji, Pavan
and Keyes, David",
title="An Overview of MPI Characteristics of Exascale Proxy Applications",
booktitle="High Performance Computing",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="217--236",
abstract="The scale of applications and computing systems is tremendously increasing and needs to increase even more to realize exascale systems. As the number of nodes keeps growing, communication has become key to high performance.",
isbn="978-3-319-58667-0"
}

@article{osti-1482870,
title = {FY18 Proxy App Suite Release. Milestone Report for the ECP Proxy App Project},
author = {Richards, David F. and Aaziz, Omar and Cook, Jeanine and Finkel, Hal and Homerding, Brian and McCorquodale, Peter and Mintz, Tiffany and Moore, Shirley and Bhatele, Abhinacv and Pavel, Robert},
abstractNote = {The ECP Proxy App Team released version 2.0 of the ECP Proxy App suite on September 28, 2018. The new version includes 5 new proxies and adds or improves coverage in several areas, most notably graph analytics and communication patterns. Proxy apps are used heavily by the PathForward projects, and several of the new proxies are being eagerly adopted. Over the last year we have observed vendors are becoming more sophisticated in their use of proxy apps. However, there is still progress to be made, especially in the area of ensuring that representative problem specifications are used. The Proxy App Team is currently working on such specifications and will be publishing them on our website in the near future.},
doi = {10.2172/1482870},
journal = {},
place = {United States},
year = {2018},
month = {10}
}
