\clearpage%
{\footnotesize\begin{landscape}%
\begin{longtable}[htb]{r|c|c|c|c|c|c|c|c|c|c}%
\caption{Q26: Is MPI providing all the communication semantics required by your application? If not, what is missing?}%
\label{tab:Q26-mans} \\%
\hline%
Multi-Answer & overall & FR & GR & IT & UK & eu & JP & RU & US & others \\
 \hline%
\endfirsthead%
\multicolumn{11}{r}{(continued from the previous page)}\\%
\hline%
Multi-Answer & overall & FR & GR & IT & UK & eu & JP & RU & US & others \\
 \hline%
\endhead%
\hline%
(total) & 733 & 104 & 135 & 48 & 57 & 117 & 60 & 86 & 52 & 74 \\%
\hline%
\multicolumn{11}{r}{(continue to the next page)}\\%
\endfoot%
\hline%
(total) & 733 & 104 & 135 & 48 & 57 & 117 & 60 & 86 & 52 & 74 \\%
\hline%
\endlastfoot%
\hline%
{MPI provides all} & 234 & 27 & 54 & 19 & 22 & 31 & 11 & 42 & 12 & 16 \\%
{Additional opt} & 72 & 12 & 12 & 3 & 2 & 9 & 7 & 8 & 3 & 16 \\%
{Latency hiding} & 61 & 10 & 9 & 4 & 0 & 15 & 7 & 6 & 4 & 6 \\%
{Resilience} & 61 & 12 & 14 & 4 & 4 & 5 & 5 & 7 & 4 & 6 \\%
{Another API} & 56 & 10 & 6 & 5 & 3 & 10 & 7 & 6 & 2 & 7 \\%
{End-points} & 33 & 3 & 4 & 5 & 2 & 3 & 4 & 2 & 4 & 6 \\%
{Resilience, Additional opt} & 29 & 2 & 7 & 3 & 2 & 9 & 1 & 2 & 3 & 0 \\%
{Latency hiding, Additional opt} & 21 & 1 & 5 & 0 & 1 & 3 & 5 & 2 & 1 & 3 \\%
{Latency hiding, Resilience} & 12 & 5 & 3 & 0 & 1 & 0 & 1 & 0 & 2 & 0 \\%
{End-points, Additional opt} & 10 & 1 & 2 & 0 & 1 & 1 & 1 & 1 & 3 & 0 \\%
{Latency hiding, Resilience, Additional opt} & 9 & 2 & 1 & 0 & 1 & 4 & 0 & 0 & 0 & 1 \\%
{Latency hiding, End-points} & 9 & 1 & 1 & 0 & 1 & 3 & 1 & 1 & 1 & 0 \\%
{Resilience, Another API} & 8 & 2 & 0 & 0 & 0 & 5 & 0 & 0 & 1 & 0 \\%
{Latency hiding, End-points, Resilience} & 8 & 2 & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 3 \\%
{Another API, MPI provides all} & 7 & 3 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 1 \\%
{Latency hiding, End-points, Resilience, Another API} & 7 & 0 & 0 & 0 & 7 & 0 & 0 & 0 & 0 & 0 \\%
{Latency hiding, Another API} & 6 & 0 & 2 & 0 & 1 & 1 & 0 & 0 & 1 & 1 \\%
{Resilience, Additional opt, Another API} & 6 & 1 & 1 & 1 & 1 & 0 & 0 & 2 & 0 & 0 \\%
{Additional opt, Another API} & 5 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 2 \\%
{End-points, Resilience, Additional opt} & 5 & 1 & 0 & 0 & 1 & 0 & 2 & 0 & 1 & 0 \\%
{Latency hiding, End-points, Resilience, Additional opt} & 4 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 1 \\%
{End-points, Another API} & 3 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\%
{Latency hiding, Resilience, Additional opt, Another API} & 3 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\%
{End-points, Resilience} & 3 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\%
{I do not know} & 2 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\%
{Latency hiding, Resilience, Another API} & 2 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\%
{Additional opt, MPI provides all} & 2 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\%
{Latency hiding, End-points, Additional opt} & 2 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 \\%
{Good integration with modern C++, including coroutines/fibers/async/future as well as object (de)serialization} & 2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 2 & 0 \\%
{RMA operation ordering} & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\%
{Latency hiding, Additional opt, Another API} & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\%
{Latency hiding, End-points, Another API} & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\%
{Latency hiding, Additional opt, notified access} & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\%
{Additional opt, Another API, Accelerator triggered communication} & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\%
{RPC like API} & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\%
{End-points, Resilience, MPI provides all} & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\%
{Latency hiding, End-points, Expose de-aggregated (eg Request-based) REMOTE completion for individual MPI\_PUT operations. Allow Accumulate/Fetch-op to concurrently perform different computational operations on a single location (eg fetch-add racing with swap or compare-swap)} & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\%
{Using custom data types safely to pack up a struct is far too complicated.} & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\%
{I don't know} & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\%
{I would like to increase the maximum number of communicators.} & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\%
{Additional opt, Better debuggability.} & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\%
{Resilience, MPI provides all} & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\%
{End-points, Additional opt, Another API} & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\%
{OpenMP like pragma/comment calls that can easily be switched off when not needed. Of course, not needed in parallelisation aware languages like Fortran, which brings me to another point: ability to influence how MPI interfaces with co-arrays.} & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\%
{RPC, reasons mentioned above.} & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\%
{truely(!) one sided comm, see one before} & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\%
{dynamicportability of MPI programs virtual commuication ID} & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\%
{Resilience, Additional opt, Another API, MPI provides all} & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\%
{Sparse Alltoall routines} & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\%
{Latency hiding, Additional opt, MPI provides all} & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\%
{I am not sure about this} & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\%
{I don't know.} & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\%
{Not fully aware of the issue} & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\%
{I hope simply short latency.  It is not hiding.} & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\%
{Additional opt, many interfaces uses int types for size, whereas they should use size\_t to deal with buffers longer than 2\^31 bytes} & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\%
{Latency hiding, Additional opt, One sided with completation signal (see OpenSHMEM)} & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\%
{no clue} & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\%
{Latency hiding, Additional opt, Custom reduction operations of variable data size} & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\%
{Latency hiding, A better interaction with OpenMP tasks} & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\%
{Latency hiding, End-points, Additional opt, Another API} & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\%
{Non-blocking collectives} & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\%
{Debugging capabilities, i.e. message sequence traces, type matching, etc.} & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\%
{A simple API would be good, not everybody wants all the expressiveness in MPI} & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\%
{better async IO} & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\%
{Latency hiding, End-points, Multiplatform support - this is technically not the specification, but the implementations, most of which are becoming VERY single-platform} & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\%
{QOS by communicator - I know which traffic I wish to prioritise from a latency perspective!} & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\%
{Another API, MPI provides all, Exceptions support in C++} & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\%
{Unspecified message size} & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\%
{End-points, Resilience, Another API} & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\%
{Latency hiding, End-points, Need a way to tell when processes first go out of sync.} & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\%
{Another API, Exception handling} & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\%
{Don't know} & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\%
{Too stupid to give an educated opinion} & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\%
{Resilience, Additional opt, Possibility to dynamically change the communicators topology upon machine failure or addition} & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\%
{Another API, Better C++ support (with testing and performance assessment)} & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\%
{load balancing} & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\%
{Building inter-communicator is too slow} & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\%
{client-server connection for coupled application, more versatility in launching multiple applications in parallel than MPMD} & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\%
{Additional opt, Another API, Often it is not raw performance which is most important, but rather how to get a particular job done.  Last time I checked (a very long time ago) it was somehow difficult to write a process pool, something easily done with ordinary threading (I may be ignorant though)} & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\%
{Latency hiding, End-points, Resilience, Additional opt, Another API} & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\%
\hline%
\end{longtable}%
\end{landscape}}%
\clearpage%
